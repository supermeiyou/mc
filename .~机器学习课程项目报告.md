# 机器学习课程项目报告

小组成员：

​	20245227133，韩宗诚，研究方向：多模态；贡献：

​	20245227135，孙浩然，研究方向：多模态；贡献：

## 一、问题介绍

### 问题描述

随着智能家居与物联网技术的不断发展，家庭电力消耗监测与管理成为节能减排、降低用电成本和实现智能化生活的重要环节。通过对家庭电力消耗进行细致的数据采集和建模分析，不仅有助于居民了解自身用电行为，还能为电力公司合理调度和预测电力负荷、平衡供需提供技术支持。家庭电力消耗受多种因素影响，例如季节变换、节假日、家庭成员行为模式、用电设备种类及气象条件等，这使得准确预测具有较大挑战性和现实意义。对家庭电力消耗进行多变量时间序列预测，可以帮助用户及时发现异常用电，合理安排用电时间，降低峰值负荷，从而节省电费和提升能源利用效率。同时，精确的用电预测对智能电网的动态调度、可再生能源接入与分布式能源管理等也有重要推动作用。

### 数据说明

典型数据集为UCI Machine Learning Repository公开的“Individual household electric power consumption”数据集，可在https://archive.ics.uci.edu/dataset/235/individual+household+electric+power+consumption获取，采集自法国一户家庭，记录时间跨度从2006年12月到2010年11月，数据粒度为每一分钟，包括全屋有功功率、无功功率、电流、电压、各路子表的能耗等多个变量。我们通常会以每天为单位对原始数据进行汇总，同时可融合天气等外部因素作为输入变量，构建多变量时间序列预测模型。数据以月为基础汇总数据，然后提取并添加相应的信息。天气信息可在下述网站获取：https://www.data.gouv.fr/fr/datasets/donnees-climatologiques-de-base-mensuelles。

### 任务要求

根据所提供的数据对未来总有功功率进行预测。基于过去90天的数据曲线来预测未来90天（短期预测）和365天（长期预测）两种长度的变化曲线（需要分别训练，即长期预测的模型参数不能用于短期预测）。按照方法分为三部分：
1.使用 LSTM 模型进行预测；
2.使用 Transformer 模型进行预测；
3.使用自己提出的改进模型进行预测

最后使用两种评价标准进行测试，即均方误差（MSE）与平均绝对误差（MAE）。至少进行五轮实验，并对结果取平均值，同时提供标准差（std）以评估结果的稳定性。

## 二、模型

### LSTM 模型

LSTM (Long Short-Term Memory)也称长短时记忆结构,它是传统RNN 的变体,与经典RNN 相
比能够有效捕捉长序列之间的语义关联,缓解梯度消失或爆炸现象.同时LSTM 的结构更复杂,
它的核心结构可以分为四个部分：细胞状态、输入门、遗忘门、输出门。
LSTM 模型的总体结构如下图所示。



遗忘门（forget gate）
遗忘门的作用是决定细胞状态中哪些信息需要被保留，哪些信息需要被丢弃。它接收当前输
入和上一时刻的隐藏状态作为输入，通过一个Sigmoid 激活函数将其映射到0 到1 之
间的值。其中，接近0 的值表示对应的细胞状态信息将被遗忘，接近1 的值表示信息将被
保留。遗忘门的计算公式如下：
$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$
其中 $f_t$ 和 $b_f$ 分别是遗忘门的权重矩阵和偏置向量。

输入门（Input Gate）
输入门负责控制当前输入中有多少信息将被更新到细胞状态中。它同样接收ℎ�−1和��作为
输入，通过Sigmoid 函数计算出一个更新比例，同时通过一个Tanh 激活函数对当前输入
进行变换，然后将两者相乘得到需要更新到细胞状态中的信息。输入门的计算公式如下：

## 三、结果与分析

### 预测任务：

根据所提供的数据对未来总有功功率进行预测。基于过去90天的数据曲线来预测未来90天（短期预测）和365天（长期预测）两种长度的变化曲线（需要分别训练，即长期预测的模型参数不能用于短期预测）

### 1.使用LSTM进行预测

#### 使用90天数据预测未来90天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：500

实验1：

![LSTM-90-1](C:\Users\saoran\Desktop\work\image\LSTM-90-1.png)

实验2：

![LSTM-90-2](C:\Users\saoran\Desktop\work\image\LSTM-90-2.png)

实验3：

![LSTM-90-3](C:\Users\saoran\Desktop\work\image\LSTM-90-3.png)

实验4：

![LSTM-90-4](C:\Users\saoran\Desktop\work\image\LSTM-90-4.png)

实验5：

![LSTM-90-5](C:\Users\saoran\Desktop\work\image\LSTM-90-5.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709103539842](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709103539842.png)

#### 使用90数据预测未来365天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：1000

实验1：

![image-20250709111237501](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111237501.png)

实验2：

![image-20250709111244593](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111244593.png)

实验3：

![image-20250709111253316](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111253316.png)

实验4：

![image-20250709111301179](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111301179.png)

实验5：

![image-20250709111415243](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111415243.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709111409395](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111409395.png)

### 2.使用transformer进行预测：

#### 使用90天数据预测未来90天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：500

实验1：

![image-20250709111600693](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111600693.png)

实验2：

![image-20250709111649828](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111649828.png)

实验3：

![image-20250709111727164](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111727164.png)

实验4：

![image-20250709111815482](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709111815482.png)

实验5：

![image-20250709112304390](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112304390.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709112324508](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112324508.png)

#### 使用90天数据预测未来365天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：1000

实验1：

![image-20250709112251600](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112251600.png)

实验2：

![image-20250709112357809](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112357809.png)

实验3：

![image-20250709112408974](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112408974.png)

实验4：

![image-20250709112414401](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112414401.png)

实验5：

![image-20250709112502962](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112502962.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709112519997](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112519997.png)

### 3.使用CNN+LSTM进行预测：

#### 使用90天数据预测未来90天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：500

实验1：

![image-20250709112708500](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112708500.png)

实验2：

![image-20250709112717384](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112717384.png)

实验3：

![image-20250709112725654](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112725654.png)

实验4：

![image-20250709112734455](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112734455.png)

实验5：

![image-20250709112753406](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112753406.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709112809442](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112809442.png)

#### 使用90天数据预测未来365天：

**实验参数设置：**lr（学习率）：0.001；epoch（轮次）：1000

实验1：

![image-20250709112905676](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112905676.png)

实验2：

![image-20250709112914183](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112914183.png)

实验3：

![image-20250709112932391](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112932391.png)

实验4：

![image-20250709112952833](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709112952833.png)

实验5：

![image-20250709113010082](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709113010082.png)

五次实验MSE，MAE与标准差值计算：

![image-20250709113022583](C:\Users\saoran\AppData\Roaming\Typora\typora-user-images\image-20250709113022583.png)

### 4.实验结果分析：

#### 对于使用90天数据预测未来90天：

**LSTM**捕捉趋势能力中等，某些高频变动未能拟合，偶尔出现时间滞后；

**Transformer**：曲线相对平滑，捕捉主趋势能力较强，但对突变反应不足（如峰谷变化被平滑化）；

**CNN+LSTM**：整体走势更加贴近原始值，细节拟合较LSTM更好，但存在高估（预测值整体偏高）。

#### 结果分析：

**LSTM**表现稳定，适用于趋势建模，但对异常点处理不够；

**Transformer**在稳定性上更优（标准差低），但欠拟合高频波动，可能因位置编码无法充分建模周期性；

**CNN+LSTM**结合局部特征提取与长期依赖捕捉，虽然MAE最高，但误差分布稳定（标准差最小），整体结构最强健。

#### 对于使用90天数据预测未来365天：

**LSTM**：后期预测误差放大，趋势模糊，存在明显偏移；

**Transformer**：对整体波动区间建模较成功，尤其在季节性变化上较准确，但预测值略偏中值；

**CNN+LSTM**：趋势覆盖范围广，但上下波动幅度大，存在高频噪声，某些时段过度拟合。

#### 结果分析：

**Transformer**在长期预测中相对最优，MSE最低且趋势拟合更自然，适合捕捉大周期性模式；

**LSTM**误差波动小但预测偏差大，时间跨度加长后遗忘严重；

**CNN+LSTM**误差浮动范围最大，需进一步调参防止过拟合。

## 四、讨论

本研究围绕家庭总有功功率预测任务，比较了三类主流时间序列模型：LSTM、Transformer 与 CNN+LSTM。在短期预测中，CNN+LSTM表现出最高的稳定性与精度，能够更好地捕捉局部波动趋势；而LSTM则在结构简洁与趋势建模方面具备一定优势。长期预测方面，Transformer模型凭借其强大的长期依赖建模能力，在整体波动控制与趋势追踪上表现最佳，LSTM与CNN+LSTM则因信息遗忘或过拟合而效果相对逊色。

从模型特性来看，LSTM适合处理周期性不强、任务需求简单的场景；Transformer适用于建模复杂周期结构与长跨度趋势；而CNN+LSTM则将局部模式提取与序列建模结合，适合短期高频预测，但需通过正则化手段控制过拟合风险。

未来的研究可从两个方面展开优化：一方面，可引入外部变量如天气、节假日等，提升模型对现实因素的适应性与泛化能力；另一方面，建议探索模型结构的融合，如将LSTM的稳定性与Transformer的全局建模能力结合，构建更具鲁棒性的混合预测模型（Hybrid Model）。同时，在训练过程中引入早停、正则化、学习率调度等机制也是解决过拟合问题的关键策略。

综上，针对不同预测任务特性选择合适的模型结构，并配合优化手段进行细致调参，是提升电力预测精度与稳定性的核心路径。

## 参考文献

[1] https://blog.csdn.net/qq_47885795/article/details/143462299
[2] https://blog.csdn.net/weixin_39653948/article/details/105431099
[3] https://datac.blog.csdn.net/article/details/105928752?fromshare=blogdetail&sharetype=blogdetail&sharerId=105928752&sharerefer=PC&sharesource=weixin_44709585&sharefrom=from_link